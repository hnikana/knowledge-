{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Gamam parameters in xgboost ?\n",
    "\n",
    "    Gamma Parm is kind of lagrangian multiplier , it puts some weight to penalize the constrain , when the min-leaf- weight and max_depth is a constraint withn each tree , Gamma is for across trees. \n",
    "    It is a pseudo-regularization hyperparameter in gradient boosting.\n",
    "    Mathematically you call \"Gamma\" the \"Lagrangian multiplier\" (complexity control).\n",
    "    Gamma: mathematically, this is known as the Lagrangian Multiplier, and its purpose is complexity control. It is a pseudo-regularization term for the loss function; and it represents by how much the loss has to be reduced when considering a split, in order for that split to happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance in decision tress (RF & xgb )\n",
    "Feature importance is calculated as the decrease in node impurity weighted by the probability of reaching that node. The node probability can be calculated by the number of samples that reach the node, divided by the total number of samples. The higher the value the more important the feature.\n",
    "Implementation in Scikit-learn\n",
    "For each decision tree, Scikit-learn calculates a nodes importance using Gini Importance, assuming only two child nodes (binary tree):\n",
    "\n",
    "ni sub(j)= the importance of node j\n",
    "w sub(j) = weighted number of samples reaching node j\n",
    "C sub(j)= the impurity value of node j\n",
    "left(j) = child node from left split on node j\n",
    "right(j) = child node from right split on node j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How feature importance is calculated in trees or random forest ?\n",
    "\n",
    "# Random fores \n",
    "\n",
    "$ n_{ij} =w_j* C_j - w_{left(j)} * C_{left(j)} -  w_{right(j)} * C_{right(j)}  $\n",
    "\n",
    "$ n_{ij} $ = the importance of node j \n",
    "\n",
    "$ w_j $ = weighted number of samples reaching node j  \n",
    "\n",
    "$ C_j $ = the impurity value of node j \n",
    "\n",
    "$ left(j)$  = child node from left split on node j  \n",
    "\n",
    "$ right(j)$   = child node from right split on node j \n",
    "\n",
    "## The importance for each feature on a decision tree is then calculated as:\n",
    "\n",
    "# $ f_{ij} = \\frac {\\sum_{i} n_{ij} : \"the weighted gain (reduced gini) over all the splits of feature j in tree i'}{\\sum_{k} n_{ik} : all nodes } $ \n",
    "\n",
    "# $ f_i $ = the importance of feature i\n",
    "# $  n_{ij} $= the importance of node j\n",
    "\n",
    "# $ norm f_{ij} = \\frac {f_{ij}}{\\sum_{j : trees } f_{ij}} $ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
