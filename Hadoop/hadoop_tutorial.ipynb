{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hadoop Tutorial \n",
    "\n",
    "**Terminology**\n",
    "\n",
    "    PayLoad − Applications implement the Map and the Reduce functions, and form the core of the job.\n",
    "\n",
    "    Mapper − Mapper maps the input key/value pairs to a set of intermediate key/value pair.\n",
    "\n",
    "    NamedNode − Node that manages the Hadoop Distributed File System (HDFS).\n",
    "\n",
    "    DataNode − Node where data is presented in advance before any processing takes place.\n",
    "\n",
    "    MasterNode − Node where JobTracker runs and which accepts job requests from clients.\n",
    "\n",
    "    SlaveNode − Node where Map and Reduce program runs.\n",
    "\n",
    "    JobTracker − Schedules jobs and tracks the assign jobs to Task tracker.\n",
    "\n",
    "    Task Tracker − Tracks the task and reports status to JobTracker.\n",
    "\n",
    "    Job − A program is an execution of a Mapper and Reducer across a dataset.\n",
    "\n",
    "    Task − An execution of a Mapper or a Reducer on a slice of data.\n",
    "\n",
    "    Task Attempt − A particular instance of an attempt to execute a task on a SlaveNode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is not a language but a distributed processing engine. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Componenet of HAddop : \n",
    "1. HDFS - Data Storage \n",
    "2. YARN - Resource Manager \n",
    "3. Map Reduce Processing - Data Processing \n",
    "4. Haddop Common utilities (spark ,, hive , flueme ,,,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer and Action :**\n",
    "\n",
    "    transformation API are the ones that we can process the data and Actions are those that we can preview the data or save the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lazy evaluation :\n",
    "    \n",
    "    transformation does not perform any job but actions will \n",
    "    \n",
    "DAG : Directed acyclic graph\n",
    "    \n",
    "    Every transformation will be collected in Dag and when an action is called , then the DAG runs those transformation \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note , when we are working with Hadoop in out local terminal , command lines start with haddop df <br>\n",
    "while if we want to access our data iun local machine like normal \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sparkcontext or sc is a gateway to spark \n",
    "help(sc) acn gives us the method and APIs\n",
    "we can read by sc.textfile and it create **RDD** object.\n",
    "\n",
    "An RDD stands for Resilient Distributed Datasets. It is Read-only partition collection of records. RDD is the fundamental data structure of Spark. It allows a programmer to perform in-memory computations on large clusters in a fault-tolerant manner. Thus, speed up the task. Follow this link to learn Spark RDD in great detail.https://data-flair.training/blogs/spark-rdd-tutorial/\n",
    "\n",
    "Unlike an RDD, data organized into named columns. For example a table in a relational database. It is an immutable distributed collection of data. DataFrame in Spark allows developers to impose a structure onto a distributed collection of data, allowing higher-level abstraction. Follow this link to learn Spark DataFrame in detail.https://data-flair.training/blogs/apache-spark-sql-dataframe-tutorial/\n",
    "\n",
    "**Data Formats**\n",
    "\n",
    "    RDD – It can easily and efficiently process data which is structured as well as unstructured. But like Dataframe and DataSets, RDD does not infer the schema of the ingested data and requires the user to specify it.\n",
    "    \n",
    "    DataFrame – It works only on structured and semi-structured data. It organizes the data in the named column. DataFrames allow the Spark to manage schema.\n",
    "    \n",
    "    DataSet – It also efficiently processes structured and unstructured data. It represents data in the form of JVM objects of row or a collection of row object. Which is represented in tabular forms through encoders.\n",
    "    \n",
    "    \n",
    "**Data Sources API**\n",
    "    \n",
    "    RDD – Data source API allows that an RDD could come from any data source e.g. text file, adatabase via JDBC etc. and easily handle data with no predefined structure.\n",
    "    \n",
    "    DataFrame – Data source API allows Data processing in different formats (AVRO, CSV, JSON, and storage system HDFS, HIVE tables, MySQL). It can read and write from various data sources that are mentioned above.\n",
    "    \n",
    "    DataSet – Dataset API of spark also support data from different sources.\n",
    "    \n",
    "    \n",
    "**Immutability and Interoperability**\n",
    "\n",
    "    RDD – RDDs contains the collection of records which are partitioned. The basic unit of parallelism in an RDD is called partition. Each partition is one logical division of data which is immutable and created through some transformation on existing partitions. Immutability helps to achieve consistency in computations. We can move from RDD to DataFrame (If RDD is in tabular format) by toDF() method or we can do the reverse by the .rdd method. Learn various RDD Transformations and Actions APIs with examples.\n",
    "    DataFrame – After transforming into DataFrame one cannot regenerate a domain object. For example, if you generate testDF from testRDD, then you won’t be able to recover the original RDD of the test class.\n",
    "    DataSet – It overcomes the limitation of DataFrame to regenerate the RDD from Dataframe.\n",
    "    Datasets allow you to convert your existing RDD and DataFrames into Datasets.\n",
    "    \n",
    "**What does Immutablity mean in data structure? \n",
    "    \n",
    "    They cannot be changed , element schanged , removed , added etc . \n",
    "    If you want to do so , you have to create a new one and modification to them is not inplace \n",
    "    what is the point ? BECAUSE THEY ARE NOT CHANGEABLE < THREADS AND FORKS CAN SHARE THEM ALL !!!!\n",
    "    \n",
    "    \n",
    "**Schema Projection**\n",
    "\n",
    "    RDD – In RDD APIs use schema projection is used explicitly. Hence, we need to define the schema (manually).\n",
    "    DataFrame – Auto-discovering the schema from the files and exposing them as tables through the Hive Meta store. We did this to connect standard SQL clients to our engine. And explore our dataset without defining the schema of our files. ==> reading from a sql , it copy the sql schema \n",
    "    DataSet – Auto discover the schema of the files because of using Spark SQL engine.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO , with SparkContext we can read text file into RDD data structures . <br>\n",
    "Then we can perform actions on them , show() , take , ... or Collect() . Collect is Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data. <br>\n",
    "\n",
    "https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds \n",
    "\n",
    "how to read data to rdd and map transformer and reduce actions on rdd. \n",
    "\n",
    "**Understanding closures** \n",
    "\n",
    "    One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster. RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses foreach() to increment a counter, but similar issues can occur for other operations as well.\n",
    "\n",
    "**Example**\n",
    "    \n",
    "    Consider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in local mode (--master = local[n]) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):\n",
    "    \n",
    "    lacal vs Cluster mode:\n",
    "    Because the job is running on clusters , and the glola l variabel is local in memory then every time each cluster update it with its own value and it makes it very kiri o tokhmi \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SQLCONTEXT**\n",
    "\n",
    "    However by todf()we can change RDD to DataFRame but with SQLcontext we can read data frames from different formats and it inferes the schema from it by itself from Json , ORC , CSV , and Paraquet\n",
    "    \n",
    "    Load and read can do that , in load the type is an arguknet however in read is a method read.json , read.textread.orc, read.parquet \n",
    "\n",
    "**Parquet** is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON, supported by many data processing systems. Spark SQL provides support for both reading and writing parquet files that automatically capture the schema of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
